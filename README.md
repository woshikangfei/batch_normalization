 
# 批归一化
批归一化是一种提高神经网络性能和稳定性的技术。想法是归一化层输入，使它们的均值为 0 和方差为 1，类似于我们归一化网络输入的方法。批归一化对于 DCGAN 正常运作很必要。


# 批次规范化 

1. （这是什么？）（＃理论）
2. （有什么好处？）（＃好处）
3. [如何将它添加到网络？]（＃implementation_1）
4. （我们来看看吧！）（＃demo）
5. [你在躲藏什么？]（＃implementation_2）

#  什么是批次规范化？<a id='theory'> </a>

Sergey Ioffe和Christian Szegedy的2015年“批量规范化：通过减少内部协变量加速深度网络培训”（https://arxiv.org/pdf/1502.03167.pdf ）引入了批量归一化。这个想法是，我们将输入规范化为网络内的_layers，而不是将输入归一化为网络。它被称为“批量”归一化，因为在训练期间，我们通过使用当前小批量的值的均值和方差来对每一层的输入进行归一化。

为什么这可能有帮助？那么，我们知道将输入规范化为_network_有助于网络学习。但是一个网络是一系列层，其中一层的输出成为另一层的输入。这意味着我们可以将神经网络中的任何一层都视为较小网络的_first_层。

例如，想象一个3层网络。将其视为具有输入，层和输出的单个网络，而不是将层1的输出视为双层网络的输入。这个双层网络将由我们原始网络中的第2层和第3层组成。

类似地，层2的输出可以被认为是仅由层3组成的单层网络的输入。

当你想到这样 - 像一系列的神经网络互相馈送 - 那么很容易想象如何规范化每一层的输入将有所帮助。这就像将任何其他神经网络的输入规范化一样，但是您正在每个层（子网络）进行。

除了直观的原因，还有很好的数学原因，为什么它有助于网络学习更好。它有助于打击作者所谓的内在协变量。这个讨论最好在[在论文中]（https://arxiv.org/pdf/1502.03167.pdf ）和[深度学习]（http://www.deeplearningbook.org ）中处理，您可以在网上阅读的书籍Ian Goodfellow，Yoshua Bengio和Aaron Courville。具体来说，请查看[第8章：深度模型训练优化]（http://www.deeplearningbook.org/contents/optimization.html ）的批量归一化部分。



# 批次归一化的好处<a id="benefits"> </a>

批量归一化优化网络培训。已被证明有几个好处：
1. **网络训练更快** - 每次训练_iteration_实际上会慢一些，因为在正向传球期间的额外计算以及在反向传播过程中训练的附加超参数。然而，它应该更快地收敛，所以培训应该总体上更快。
2. **允许更高的学习率** - 渐变下降通常需要较小的学习率才能使网络收敛。随着网络的深入，它们的渐变在反向传播过程中变小，因此需要更多的迭代。使用批量归一化使我们能够使用更高的学习率，这进一步提高了网络训练的速度。
3. **使权重更容易初始化** - 重量初始化可能很困难，而在创建更深的网络时更加困难。批量归一化似乎使我们对选择我们的初始起始重量要小心些。
4. **使更多的激活功能可行** - 某些激活功能在某些情况下无法正常工作。 Sigmoids很快失去了梯度，这意味着它们不能在深层网络中使用。而且，ReLUs在训练过程中经常消失，在那里他们完全停止学习，所以我们需要注意它们的价值范围。因为批量归一化调节进入每个激活函数的值，所以在深层网络中似乎不能正常工作的非线性实际上变得可行。
5. **简化更深层网络的创建** - 由于上面列出的前4项，使用批量归一化时，更容易构建和更快地训练更深层次的神经网络。而且，已经表明，更深层的网络通常会产生更好的结果，所以这是非常好的。
6. **提供一些规则化** - 批处理标准化为您的网络增加了一点噪音。在某些情况下，例如Inception模块中，批处理规范化已被显示为工作以及退出。但一般来说，将批处理规范化作为一个额外的正则化，可能允许您减少您可能添加到网络的一些辍学。
7. **可以给出更好的结果** - 一些测试似乎显示批量归一化实际上提高了培训结果。然而，这真的是一个优化，以帮助训练更快，所以你不应该认为它是一种使您的网络更好的方式。但是，由于它可以让您更快地训练网络，这意味着您可以更快地迭代更多的设计。它还可以让您构建更深层网络，这通常更好。所以当你考虑一切的时候，如果你用批量规范化建立你的网络，你可能会得到更好的结果。


如您所见，现在我们使用估计的人口平均值和差异，我们得到97％的准确性。这意味着它在200个样本中的194个中正确地猜到了 - 对于在4秒以内训练的东西来说，这并不太糟糕。 :)

# 其他网络类型的注意事项

该笔记本显示了具有完全连接层的标准神经网络中的批量归一化。您也可以在其他类型的网络中使用批量规范化，但有一些特殊的注意事项。

### ConvNets

卷积层由多个特征图组成。 （记住，卷积层的深度是指其特征图的数量）。每个特征图的权重在所有输入到该层的输入中共享。由于这些差异，批量标准化卷积层需要每个特征图的批次/总体均值和方差，而不是层中的每个节点。

当使用`tf.layers.batch_normalization`时，一定要注意你的卷积尺寸的顺序。
具体来说，如果您的图层具有第一个通道而不是最后一个，则可能需要为“axis”参数设置一个不同的值。

在我们的低级实现中，我们使用以下行来计算批量均值和方差：
```python
batch_mean，batch_variance = tf.nn.moments（linear_output，[0]）
```
如果我们处理一个卷积层，我们将用这样一行计算平均值和方差：
```python
batch_mean，batch_variance = tf.nn.moments（conv_layer，[0,1,2]，keep_dims = False）
```
第二个参数`[0,1,2]`告诉TensorFlow来计算每个特征图上的批次平均值和方差。 （三个轴是批次，高度和宽度）。将`keep_dims`设置为`False`会告诉`tf.nn.moments`不返回与输入大小相同的值。具体来说，它确保我们得到每个特征图的一个均值/方差对。

### RNNs

批量归一化也可以与循环神经网络一起使用，如2016年的“经常性批次规范化”（https://arxiv.org/abs/1603.09025 ）中所示。实现更多的工作，但基本上包括计算每个时间步长的方法和方差，而不是每层。您可以找到一个例子，其中有人扩展`tf.nn.rnn_cell.RNNCell`以在[此GitHub repo]（https://gist.github.com/spitis/27ab7d2a30bbaf5ef431b4a02194ac60 ）中包括批处理规范化。

